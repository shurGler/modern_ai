(window.webpackJsonp=window.webpackJsonp||[]).push([[28],{486:function(s,a,e){"use strict";e.r(a);var t=e(11),n=Object(t.a)({},(function(){var s=this,a=s._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"开发大语言模型需要什么"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#开发大语言模型需要什么"}},[s._v("#")]),s._v(" 开发大语言模型需要什么？")]),s._v(" "),a("div",{staticClass:"custom-block note"},[a("p",{staticClass:"custom-block-title"},[s._v("笔记")]),s._v(" "),a("p",[s._v("本文主要内容来自论文 "),a("a",{attrs:{href:"https://arxiv.org/abs/2303.18223",target:"_blank",rel:"noopener noreferrer"}},[s._v("A Survey of Large Language Models"),a("OutboundLink")],1),s._v("。")])]),s._v(" "),a("p",[s._v("了解完大语言模型的原理之后，你可能会好奇 TA 是如何开发的。开发大语言模型的关键是什么。最近看到不少文章为了流量，甚至连 5G 通讯都说成了是开发大语言模型的关键 😂")]),s._v(" "),a("p",[s._v("其实从前面的原理介绍，不难看出，大语言模型的其中一个关键点是数据。")]),s._v(" "),a("h2",{attrs:{id:"关键一-数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#关键一-数据"}},[s._v("#")]),s._v(" 关键一：数据")]),s._v(" "),a("p",[s._v("训练数据主要是所谓的语料库。今天的很多语言模型的语料库主要有以下几种：")]),s._v(" "),a("ul",[a("li",[s._v("Books：BookCorpus 是之前小语言模型如 GPT-2 常用的数据集，包括超过 11000 本电子书。主要包括小说和传记，最近更新时间是 2015 年 12 月。大一点的书籍语料库是 Gutenberg，它有 70000 本书，包括小说、散文、戏剧等作品，是目前最大的开源书籍语料库之一，最近更新时间是 2021 年 12 月。")]),s._v(" "),a("li",[s._v("CommonCrawl：这个是目前最大的开源网络爬虫数据库，不过这个数据包含了大量脏数据，所以目前常用的四个数据库是 C4、CC-Stories、CC-News 和 RealNews。另外还有两个基于 CommonCrawl 提取的新闻语料库 REALNEWS 和 CC-News。")]),s._v(" "),a("li",[s._v("Reddit Links：简单理解 Reddit 就是外国版本的百度贴吧 + 知乎。目前开源的数据库有 OpenWebText 和 "),a("a",{attrs:{href:"http://PushShift.io",target:"_blank",rel:"noopener noreferrer"}},[s._v("PushShift.io"),a("OutboundLink")],1),s._v("。")]),s._v(" "),a("li",[s._v("Wikipedia：维基百科是目前大模型广泛使用的语料库。")]),s._v(" "),a("li",[s._v("Code：一般包含 GitHub 上的开源公共代码库，或者是 StackOverflow 的数据，Google 之前发布过一个叫 BigQuery 的数据集。")])]),s._v(" "),a("p",[s._v("实际上，训练大语言模型会混合使用不同的数据，一般不会只使用单一语料库。比如 GPT-3 用的混合数据集就包括 Books、CommonCrowl、Reddit Links 和 Wikipedia。")]),s._v(" "),a("p",[s._v("从数据上看，你需要知道一个事实，中文语料在这上面占比很小。ChatGPT 能用中文回答你的问题，只是因为它的语料里有一些中英文对应的语料，然后模型就像是「懂中文了一样」，所以更可能的情况是，它先生成了英文内容，再翻译成中文。如果你去问 ChatGPT 3.5：请给我几个中文谐音梗，你应该就能理解我说的话了。")]),s._v(" "),a("p",[s._v("以下是 ChatGPT 的返回结果：")]),s._v(" "),a("div",{staticClass:"language-other line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("以下是一些中文谐音梗的例子：\n\n我蒸发了，意思是我是 Vaper 了（Vaper发音接近蒸发）。\n\n会有星期天，都得过过来，意思是星期二，星期三，星期四，星期五，星期六，都得过过来。\n\n看到你，我的内心凉了 below zero，意思是凉了半截（below zero与半截音相近）。\n\n蜀道难，难于上青天，赛道难，难于上488（488与“蜀道难”同音）。\n\n洋葱姑娘，想哭却哭不出来，因为洋葱让我流泪了（洋葱和“要哭”在中文中的发音很相似）。\n\n雨纷纷，旧故里草木深，我听闻，你始终在我身边深（“身边深”和“身边寻”谐音）。\n\n你的车开得真快，一开就是“出租”，意思是“出祖宗”（发音类似）。\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br")])]),a("h2",{attrs:{id:"关键二-算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#关键二-算法"}},[s._v("#")]),s._v(" 关键二：算法")]),s._v(" "),a("p",[s._v("有了数据之后，你还需要有算法去做计算，目前最常用的开发大语言模型的算法库有：")]),s._v(" "),a("ol",[a("li",[s._v("Transformers：这是一个使用 Transformer 架构构建的开源 Python 库。")]),s._v(" "),a("li",[s._v("DeepSpeed：是由微软开发的深度学习优化库。")]),s._v(" "),a("li",[s._v("Megatron-LM：这是由 Nvidia 开发的深度学习库。")]),s._v(" "),a("li",[s._v("JAX：它是由 Google 开发的用于高新能机器学习算法的 Python 库。")])]),s._v(" "),a("h2",{attrs:{id:"关键三-算力"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#关键三-算力"}},[s._v("#")]),s._v(" 关键三：算力")]),s._v(" "),a("p",[s._v("简单理解，算力就是计算资源，或者说硬件，OpenAI 没有说它训练 GPT-3 语言模型花了多少计算资源。但 OpenAI 的 CEO 暗示硬件成本超过一亿美元，如果我们按照 1000 美元一个 GPU 计算，它大约使用了 10 万个 GPU，以 32 位运算为准，它能提供超过 100 PFLOPS 的算力，也就是每秒 10 亿亿次运算以上，这大约是阿里云最大的数据中心的四分之一的算力。")]),s._v(" "),a("p",[s._v("注意，这还是 GPT-3 时的花费。")]),s._v(" "),a("p",[s._v("另外，我还想分享一个观点，不要以为算力会随时间的前进，就能跨越。算力永远会是制约我们瓶颈，因为我们对人工智能的要求会不断的提高。")])])}),[],!1,null,null,null);a.default=n.exports}}]);